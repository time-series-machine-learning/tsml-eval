{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bake off redux: a review and experimental evaluation of recent time series classification algorithms\n",
    "\n",
    "This is the webpage and repo package to support the paper \"Bake off redux: a review\n",
    "and experimental evaluation of recent time series classification algorithms\" published in \n",
    "[Data Mining and Knowledge Discovery](https://link.springer.com/article/10.1007/s10618-024-01022-1).\n",
    "\n",
    "Our results files are stored [here](https://github.com/time-series-machine-learning/tsml-eval/tree/main/tsml_eval/publications/y2023/tsc_bakeoff/results).\n",
    "\n",
    "> __Correction:__   \n",
    "The datasets _Covid3Month_disc_, _FloodModeling1_disc_, _FloodModeling2_disc_ and _FloodModeling3_disc_ have been fixed since the original pre-print. Unfortunately _Table 1_ and _Table C4_ retain some values from the previous versions of these datasets.    \n",
    "The correct test set sizes are _61_, _202_, _201_ and _184_ respectively for _Table 1_, and the correct accuracy values for _Table C4_ can be found [here](https://github.com/time-series-machine-learning/tsml-eval/tree/main/tsml_eval/publications/y2023/tsc_bakeoff/results/table_c4.csv). Please use the updated datasets and results in any paper sourcing results from this publication.   \n",
    "\n",
    "## Datasets\n",
    "\n",
    "The 112 UCR archive datasets are available at the [timeseriesclassification.com datasets page](http://www.timeseriesclassification.com/dataset.php).\n",
    "\n",
    "The 30 new datasets will be uploaded to [timeseriesclassification.com](http://www.timeseriesclassification.com) in due course. For now, we provide the following link:\n",
    "\n",
    "<https://drive.google.com/file/d/1vuh6mgNrNKjHr9MMRQP0J0_gGA4dE7E3/view?usp=sharing>\n",
    "\n",
    "## Install\n",
    "\n",
    "To install the latest version of the package with up-to-date algorithms, run:\n",
    "\n",
    "    pip install tsml-eval\n",
    "\n",
    "To install the package at the time of publication, run:\n",
    "\n",
    "    pip install tsml-eval==0.2.1\n",
    "\n",
    "Not all estimator dependencies are installed by default. You can install these individually as required or use the following dependency groups when installing:\n",
    "\n",
    "    pip install tsml-eval[all_extras,deep_learning]\n",
    "\n",
    "To install dependency versions used at the time of publication, use the publication requirements.txt:\n",
    "\n",
    "    pip install -r tsml_eval/publications/2023/tsc_bakeoff/static_publication_reqs.txt\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Command Line\n",
    "\n",
    "Run [run_experiments.py](https://github.com/time-series-machine-learning/tsml-eval/blob/main/tsml_eval/publications/y2023/tsc_bakeoff/run_experiments.py) with the following arguments:\n",
    "\n",
    "1. Path to the data directory\n",
    "\n",
    "2. Path to the results directory\n",
    "\n",
    "3. The name of the model to run (see [set_bakeoff_classifier.py](https://github.com/time-series-machine-learning/tsml-eval/blob/main/tsml_eval/publications/y2023/tsc_bakeoff/set_bakeoff_classifier.py), i.e. R-STSF, HC2, InceptionTime)\n",
    "\n",
    "4. The name of the problem to run\n",
    "\n",
    "5. The resample number to run (0 is base train/test split)\n",
    "\n",
    "i.e. to run ItalyPowerDemand using HIVE-COTE V2 on the base train/test split:\n",
    "\n",
    "    python tsml_eval/publications/2023/tsc_bakeoff/run_experiments.py data/ results/ HC2 ItalyPowerDemand 0\n",
    "\n",
    "### Exactly Reproducing Results\n",
    "\n",
    "To better compare to past results and publications, our results on the 112 UCR datasets use the randomly generated resamples from the Java [tsml](https://github.com/time-series-machine-learning/tsml-java) package. To use these resample with our code, a flag must be toggled in the experiments file main method and individual files for each resample must be present in the data directory. These resamples in .ts file format are available for download here:\n",
    "\n",
    "<https://drive.google.com/file/d/1V36LSZLAK6FIYRfPx6mmE5euzogcXS83/view?usp=sharing> - 112 UCR datasets using Java tsml resamples\n",
    "\n",
    "The 30 new datasets used in our experiments use the resampling available by default in our experiments file. An exception to this is ProximityForest, which is implemented in Java and uses the Java resampling as a result.\n",
    "\n",
    "We provide the resample indices used for each dataset for both Java and Python resamplers here:\n",
    "\n",
    "Python - <https://drive.google.com/file/d/1aLBP_nhnoqz075puKg30zuF3F_QBOXYM/view?usp=sharing>\n",
    "\n",
    "Java - <https://drive.google.com/file/d/1FsG7Fp74y_TpaPhJ7U066ot8A07BPhr3/view?usp=sharing>\n",
    "\n",
    "### Java Classifier Implementations\n",
    "\n",
    "Three of the classifiers used in our comparison were implemented in Java due to a lack of Python implementations which function reliably and are capable of accurately reproducing published results. These classifiers are the ElasticEnsemble, ProximityForest and TS-CHIEF. We use the implementations from the Java [tsml](https://github.com/time-series-machine-learning/tsml-java) package from revisions where they are available. We make two jar files available for download which contain the implementations of these classifiers:\n",
    "\n",
    "<https://drive.google.com/file/d/1oXxpSa5PT9sBuVAbt57TLMANv4TMEejI/view?usp=sharing> - TS-CHIEF and ProximityForest\n",
    "\n",
    "<https://drive.google.com/file/d/1Vmgg5u7SE2jmsakHVlxPxvT_AfaZ151e/view?usp=sharing> - ElasticEnsemble\n",
    "\n",
    "These jar files can be run from the command line using the following commands similar to the above Python classifiers:\n",
    "\n",
    "    java -jar tsml-ee.jar -dp=data/ -rp=results/  -cn=\"FastEE\" -dn=\"ItalyPowerDemand\" -f=0\n",
    "\n",
    "or\n",
    "\n",
    "    java -jar tsml-forest.jar -dp=data/ -rp=results/ -cn=\"ProximityForest\" -dn=\"ItalyPowerDemand\" -f=0\n",
    "\n",
    "or\n",
    "\n",
    "    java -jar tsml-forest.jar -dp=data/ -rp=results/  -cn=\"TS-CHIEF\" -dn=\"ItalyPowerDemand\" -f=0\n",
    "\n",
    "### Using Classifiers\n",
    "\n",
    "Most of our classifiers are available in the `aeon` Python package.\n",
    "\n",
    "The classifiers used in our experiments extend the `scikit-learn` interface and can also be used like their estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from aeon.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tsml.compose import SklearnToTsmlClassifier\n",
    "from tsml.datasets import load_minimal_chinatown\n",
    "\n",
    "from tsml_eval.publications.y2023.tsc_bakeoff import _set_bakeoff_classifier\n",
    "from tsml_eval.utils.estimator_validation import is_sklearn_classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T16:19:23.882374Z",
     "start_time": "2024-03-19T16:19:21.390933Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data can be loaded using whichever method is most convenient, but should be formatted as either a 3D numpy array of shape (n_samples, n_channels, n_timesteps) or a list of length (n_samples) containing 2D numpy arrays of shape (n_channels, n_timesteps).\n",
    "\n",
    "A function is available for loading from .ts files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(20, 1, 24) (20,)\n",
      "(20, 1, 24) (20,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[[ 573.,  375.,  301.,  212.,   55.,   34.,   25.,   33.,  113.,\n          143.,  303.,  615., 1226., 1281., 1221., 1081.,  866., 1096.,\n         1039.,  975.,  746.,  581.,  409.,  182.]],\n\n       [[ 394.,  264.,  140.,  144.,  104.,   28.,   28.,   25.,   70.,\n          153.,  401.,  649., 1216., 1399., 1249., 1240., 1109., 1137.,\n         1290., 1137.,  791.,  638.,  597.,  316.]],\n\n       [[ 603.,  348.,  176.,  177.,   47.,   30.,   40.,   42.,  101.,\n          180.,  401.,  777., 1344., 1573., 1408., 1243., 1141., 1178.,\n         1256., 1114.,  814.,  635.,  304.,  168.]],\n\n       [[ 428.,  309.,  199.,  117.,   82.,   43.,   24.,   64.,  152.,\n          183.,  408.,  797., 1288., 1491., 1523., 1460., 1365., 1520.,\n         1700., 1797., 1596., 1139.,  910.,  640.]],\n\n       [[ 372.,  310.,  203.,  133.,   65.,   39.,   27.,   36.,  107.,\n          139.,  329.,  651.,  990., 1027., 1041.,  971., 1104.,  844.,\n         1023., 1019.,  862.,  643.,  591.,  452.]]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load example classification dataset\n",
    "X_train, y_train = load_minimal_chinatown(\"TRAIN\")\n",
    "X_test, y_test = load_minimal_chinatown(\"TEST\")\n",
    "\n",
    "# data can be loaded from .ts files using the following function\n",
    "# from tsml.datasets import load_from_ts_file\n",
    "# X, y = load_from_ts_file(\"data/data.ts\")\n",
    "\n",
    "print(type(X_train), type(y_train))\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "X_train[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T16:19:23.898345Z",
     "start_time": "2024-03-19T16:19:23.884344Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classifiers can be built using the `fit` method and predictions can be made using `predict`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 2., 2., 1., 2.,\n       2., 2., 2.])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a TSF classifier and make predictions\n",
    "tsf = TimeSeriesForestClassifier(n_estimators=100, random_state=0)\n",
    "tsf.fit(X_train, y_train)\n",
    "tsf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T16:19:24.542613Z",
     "start_time": "2024-03-19T16:19:23.899304Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`predict_proba` can be used to get class probabilities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.92, 0.08],\n       [0.82, 0.18],\n       [0.85, 0.15],\n       [0.97, 0.03],\n       [0.85, 0.15],\n       [0.83, 0.17],\n       [0.96, 0.04],\n       [0.91, 0.09],\n       [0.89, 0.11],\n       [0.87, 0.13],\n       [0.11, 0.89],\n       [0.16, 0.84],\n       [0.52, 0.48],\n       [0.2 , 0.8 ],\n       [0.07, 0.93],\n       [0.97, 0.03],\n       [0.11, 0.89],\n       [0.  , 1.  ],\n       [0.  , 1.  ],\n       [0.35, 0.65]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsf.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T16:19:24.620729Z",
     "start_time": "2024-03-19T16:19:24.543610Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we run some of the classifiers from the publication and find the accuracy for them on our example dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.9, 0.9, 0.9, 0.85]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [\n",
    "    \"RDST\",\n",
    "    \"R-STSF\",\n",
    "    \"WEASEL-D\",\n",
    "    \"MultiROCKET-Hydra\",\n",
    "]\n",
    "\n",
    "accuracies = []\n",
    "for classifier_name in classifiers:\n",
    "    # Select a classifier by name, see set_bakeoff_classifier.py for options\n",
    "    classifier = _set_bakeoff_classifier(classifier_name, random_state=0)\n",
    "\n",
    "    # if it is a sklearn classifier, wrap it to work with time series data\n",
    "    if is_sklearn_classifier(classifier):\n",
    "        classifier = SklearnToTsmlClassifier(\n",
    "            classifier=classifier, concatenate_channels=True, random_state=0\n",
    "        )\n",
    "\n",
    "    # fit and predict\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "accuracies"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T16:19:29.650540Z",
     "start_time": "2024-03-19T16:19:24.621639Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classifier Parameters\n",
    "\n",
    "| Classifier             | Parameters                                                                                                                                                       |\n",
    "|------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1NN-DTW                | Warping window: Full                                                                                                                                             | \n",
    "| ShapeDTW               | Warping window: Full, Subsequence length: 30                                                                                                                     | \n",
    "| EE                     | Neighbourhood size limit: 0.1, Parameter size limit: 0.5                                                                                                         |\n",
    "| PF                     | Number of trees: 100, Number of splits: 5                                                                                                                        |\n",
    "| GRAIL                  | Classifier: SVM with CV, kernel: SINK, d: max(min(_n_ * 0.4), 100), 3), f: 0.99                                                                                  | \n",
    "| Catch22                | Classifier: Random Forest with 500 trees                                                                                                                         | \n",
    "| Signatures             | Classifier: Random Forest with 500 trees, Truncation depth: 4, Window: dyadic with depth 3                                                                       | \n",
    "| TSFresh                | Classifier: Random Forest with 500 trees, Parameter set: efficient, Feature extraction: FRESH                                                                    | \n",
    "| FreshPRINCE            | Classifier: Rotation Forest with 200 trees, Parameter set: comprehensive                                                                                         | \n",
    "| RSF                    | Number of trees: 100, Shapelets per node: 10                                                                                                                     | \n",
    "| STC                    | n_shapelet_samples: 10000, max_shapelets: min(10 * _n_, 1000)                                                                                                    | \n",
    "| MrSQM                  | Feature selection strategy: RS, Features per representation: 500, Candidate features per representation: 2000, SAX transformers: 0, SFA Transformers: 5          |\n",
    "| RDST                   | Number of shapelets: 10000, Normalization chance: 0.8, Alpha similarity: 0.5                                                                                     | \n",
    "| TSF                    | Number of trees: 500, Intervals per tree: sqrt(m)                                                                                                                | \n",
    "| RISE                   | Number of trees: 500, ACF lags: 100                                                                                                                              | \n",
    "| CIF                    | Number of trees: 500, Intervals per tree: sqrt(_m_)*sqrt(_d_), Attributes per tree: 8, Max interval length: _m_/2                                                | \n",
    "| DrCIF                  | Number of trees: 500, Intervals per representation (_rm_=representation length): 4+(sqrt(_rm_)*sqrt(_d_))/3, Attributes per tree: 10, Max interval length: _m_/2 | \n",
    "| STSF                   | Number of trees: 500                                                                                                                                             | \n",
    "| R-STSF                 | Classifier: Extra trees with 500 trees, Interval extraction runs: 50                                                                                             | \n",
    "| QUANT                  | Classifier: Extra trees with 200 trees, Interval depth: 6, Quantile divisor: 4                                                                                   | \n",
    "| BOSS                   | Max ensemble size: 500, Accuracy threshold: 0.92                                                                                                                 | \n",
    "| cBOSS                  | Parameter sets sampled: 250, Max ensemble size: 50                                                                                                               | \n",
    "| TDE                    | Parameter sets sampled: 250, Max ensemble size: 50                                                                                                               | \n",
    "| WEASEL v1.0            | Classifier: Logistic regression, Alphabet size: 4, Feature selection threshold: 0.05                                                                             | \n",
    "| WEASEL v2.0            | Classifier: Logistic regression, Alphabet size: 2, Max feature count: 30000                                                                                      | \n",
    "| ROCKET                 | Classifier: Ridge with cross-validation, Number of kernels: 10000                                                                                                | \n",
    "| Arsenal                | Ensemble size: 25, Number of kernels: 2000                                                                                                                       | \n",
    "| MultiROCKET            | Classifier: Ridge with cross-validation, Number of kernels: 10000                                                                                                | \n",
    "| MiniROCKET             | Classifier: Ridge with cross-validation, Number of kernels: 10000                                                                                                | \n",
    "| Hydra                  | Classifier: Ridge with cross-validation, Number of groups: 64, Kernels per group: 8,                                                                             | \n",
    "| MR-Hydra               | Classifier: Ridge with cross-validation, MR kernels: 10000, g: Hydra groups, Hydra kernels per group: 8,                                                         | \n",
    "| CNN                    | Average pooling size: 3, Batch size: 16, Kernel size: 7, Number of epochs: 2000, Number of layers: 2                                                             | \n",
    "| ResNet                 | Batch size: 64, Number of layers: 3, Number of epochs: 1500, n_residual_blocks: 3                                                                                | \n",
    "| InceptionTime          | Ensemble size: 5, Batch size: 64, Inception modules: 6, Kernel size: 40, Max pooling size: 3, Number of epochs: 1500, Number of layers: 3, Inception filters: 32 | \n",
    "| H-InceptionTime        | Ensemble size: 5, Batch size: 64, Inception modules: 6, Kernel size: 40, Max pooling size: 3, Number of epochs: 1500, Number of layers: 3, Inception filters: 32 | \n",
    "| LiteTime               | Ensemble size: 5, Batch size: 64, Kernel size: 40, Number of epochs: 1500, Inception filters: 32                                                                 | \n",
    "| TS-CHIEF               | Number of trees: 500, EE splitters: 5, RISE splitters: 100, BOSS splitters: 100                                                                                  |\n",
    "| HC1                    | Alpha: 4                                                                                                                                                         |\n",
    "| HC2                    | Alpha: 4                                                                                                                                                         | \n",
    "| RIST                   | Classifier: Extra trees with 500 trees, Number of intervals: sqrt(_m_) * sqrt(_d_) * 15 + 5, Number of shapelets: sqrt(_m_) * 200 + 5                            |"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
